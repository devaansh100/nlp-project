# Self-guided Decoding to Reduce Hallucinations in Large Language Models
Here is the code used for this paper, implemented as part of the class project for CS263: Natural Language Processing, offered by Prof. Violet Peng in Fall 2024, at UCLA.

Previous works have found that LLM represenations can be used to detect hallucinated sentences. In this work, we aim to study if those responses can be used to steer the model towards truthful generations by reducing the probability of hallucinated sequences in a beam.

# NLP_Project.ipynb
Running this notebook will, (i) get the TruthfulQA dataset, (ii) create the token-level dataset for the classifier, (iii) train the classifier, (iv) use the classifier for inference. Some auxiliary files, like the ```test_indices.npy``` will also get created. 

One of the cells modifies the transformers source code, by adding some additional lines in the generation/utils.py. This would not affect other models.

This is the main notebook, used to create generation outputs, and calculate the BLEU Scores. This notebook also saves the model outputs.

# human_eval.ipynb
Use the model outputs generated by ```NLP_Project.ipynb``` in this notebook for an interface to perform human evaluation. Once the evaluation is complete, it automatically generates the Truthfullness and Informativeness score.